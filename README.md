---

# ğŸ” Gemma Models Benchmarking Suite

A comprehensive and extensible benchmarking suite for evaluating **Gemma language models** on academic benchmarks and custom datasets.

---

## ğŸš€ Features

- âœ… Support for multiple **Gemma model variants**
- ğŸ“Š Integration with **academic benchmarks** (MMLU, GSM8K, and more)
- ğŸ“ **Custom dataset** evaluation support
- âš–ï¸ Comparative analysis with other **open-source models**
- ğŸ“ˆ Automated **report generation and visualization**
- ğŸ”Œ **Modular and extensible** design for easy integration of new benchmarks or models

---

## ğŸ› ï¸ Setup Instructions

### âœ… Prerequisites

Ensure you have the following installed:

- Python **3.10+**
- Git
- Windows OS (for now)
- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://www.anaconda.com/download/) â€“ **recommended for managing environments**

---

### ğŸ“¦ Installation

1. **Clone the repository**

```bash
git clone https://github.com/dhyaneesh/gemma-benchmarking.git
cd gemma-benchmarking
```

2. **Set up the environment**

<details>
<summary><strong>Option 1: Using Conda (Recommended)</strong></summary>

```bash
conda env create -f environment.yml
conda activate gemma-benchmark
```
</details>

<details>
<summary><strong>Option 2: Using Python venv</strong></summary>

```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
</details>

---

## âš™ï¸ Configuration

All benchmarking settings are controlled via JSON configuration files in the `configs/` directory.

- The **default configuration** is available at: `configs/default.json`
- You can create custom configs to tailor model selection, datasets, and evaluation settings.

---

## ğŸ“ˆ Usage

Run with the **default config**:

```bash
python src/main.py
```

Run with a **custom config**:

```bash
python src/main.py --config configs/custom_config.json
```

Specify models and benchmarks via CLI:

```bash
python src/main.py --models gemma-2b gemma-7b --benchmarks mmlu gsm8k
```

---

## ğŸ“ Project Structure

```
gemma-benchmarking/
â”œâ”€â”€ configs/              # Configuration files (JSON)
â”œâ”€â”€ environment.yml       # Conda environment specification
â”œâ”€â”€ logs/                 # Log files
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ results/              # Benchmark output results
â”œâ”€â”€ scripts/              # Utility scripts (e.g., dataset preparation)
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ benchmarks/       # Benchmark task implementations
â”‚   â”œâ”€â”€ models/           # Model wrappers and loading logic
â”‚   â”œâ”€â”€ utils/            # Helper utilities and tools
â”‚   â”œâ”€â”€ visualization/    # Visualization and reporting tools
â”‚   â””â”€â”€ main.py           # Entry point for benchmarking
â””â”€â”€ README.md             # You're here!
```

---

## ğŸ“Œ Roadmap

- [ ] Add support for additional Gemma model variants
- [ ] Expand academic benchmark integration
- [ ] Improve visualization and report automation
- [ ] Add leaderboard comparison with open models (e.g., LLaMA, Mistral)
- [ ] Docker support and multiplatform compatibility
- [ ] Add CLI wizard for quick setup

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

### ğŸ™Œ Contributing

Pull requests, issues, and suggestions are welcome! Please open an issue or start a discussion if youâ€™d like to contribute.

---
